# 💬 Overview
- get-in : btd paradox, digital signature susceptibility
- model of secure system
	- condition for perfect secrecy
    - shannon entroyphy
    - equivocation(모호성)
    - perfect secrecy (Priori & Posterior Probability)
    - redundancy
    - unicity distance
- complexity theroy

# 🪜 Get in
## Birthday paradox
	about 50% change of having a same birthday when there are 23 people!

P(n) = prob that at least two have a same BTD when there are n persons

P(n) = 1 - P'(n)
(P'(n) = no one has same BTD)

P(1) = 1
P(2) = 1 \* (365-1)/365
P(3) = 1 \* (365-1)/365 \* (365-2)/365



## Digital Seignature Susceptibility
susceptibility : 민감성
![](https://images.velog.io/images/yesterdaykite/post/a8a6327a-a3aa-4ab2-aa62-dc7209596796/image.png)


---
# 🍦 Model of contents
![](https://images.velog.io/images/yesterdaykite/post/f0f3667d-5018-42a1-b7fe-3a61457aa8e9/image.png)

## ✨Shannon theory
__communication theory__ of secrecy system
![](https://images.velog.io/images/yesterdaykite/post/47a1f5e9-4ff7-4dfc-af5b-04d12ebeb1fc/image.png)
similar to __"noisy channel problem"__
i.e) A user wishing to send a message over a noisy channel __must add redundancy__ to allow __detection/correction of errors__.
- noisy channel : M을 전송하는데, 노이즈에 의해 C로 변화된 M을 에러탐색과 수정을 통해 다시 M으로 수정

## ✨PERFECT SECRECY - Priori & Posterior Probability

### ✔️ in words..
암호화되기전 (pri) Key의 확률, Message의 확률과
암호화 된 후 (post) 공격자가 cipher text를 통해 가능한 key와 message를 계산하는 가능성

- __Pi : key의 확률__ : A Kj is selected from the set of all keys K with probability Pi
- __Mj : Message 의 확률__ : A msg Mj is selected from the set of all msgs M with a priori probability Qj
(i.e. data가 16bit면 2^16가지의 msg 경우가 있다.)
- C = E(Mj, Ki)
=> __C를 관찰한 Attacker 는 가능한 Mj와 Ki의 postterior probability를 계산할 수 있다. __
=> posteriori probability = __attacker's knowledge__를 represent


### ✔️ in calc..
![](https://images.velog.io/images/yesterdaykite/post/a31cb00f-dc97-4b54-bbae-6849112b00e2/image.png)
- P(M|C) = C를 얻은 attacker가 제대로 M을 해독할 확률 = prob. of M given C intercepted
- P(C|M) = M에 의해 C가 생성될 확률 = prob. of C generated by M
- P(M) = M이 선택될 확률 = prob. of M being selected
- P(C) = C가 입수될 확률 = prob. of obtaing C



### ✔️ Condition for PERFECT SECRECY
> 🤔 IF  __P(M|C) = P(M)__ for all C & M
__attacker gain NO info by intercepting C__

= c가 강탈되었을때조차 M을 (랜덤) 선택하는 확률과 같다.
= c를 강탈해도 M에 대한 정보를 얻는 것이 없다

> 🤭 So __Perfect Securecy__ is
1. __P(M|C) = P(M)__ for all C & all M
2. __P(C|M) = P(C)__ for all C & all M

> 🤭 So __Perfect System__ is
=  __as many C's as M's__
= at least one K transforming any M into these C's.

this works for guessing imppossible XXD

these are neccessary and sufficient condition !

## ✨ Shannon Entropy
### ✔️ definition
> ![](https://images.velog.io/images/yesterdaykite/post/2534ad87-158b-495f-9b3f-4ac5aa82c413/image.png) __measure of unpredictability__ in random variable
(높을 수록 예측하기 힘들다!)

### ✔️ Examples

1) Political Poll
- __Entrophy(H) is large : round 1__ - outcome of poll is not already known 'unpredictable'
	i.e.) Outcome of the poll and learning the result / gives / __some new info!__
- __Entrophy(H) is small :  round 2__ - _same poll_ performed after the first round
	i.e) outcome of the first-round poll / already known / and the second poll can be predicted.

2) Coin Flipping (fair coin case)
- Entropy(H) = 1 = 전혀 예측 불가능
- P(head) = P(tail) = no way to predict!


### ✔️ Calc..
> __Message Entrophy__
= amount of info produced / __when a message is chosen__
![](https://images.velog.io/images/yesterdaykite/post/89d790e7-dc21-4ed0-bda7-52055dab5b77/image.png)

> __Key Entropy__
= amount of info produced / __when a key is chosen__
![](https://images.velog.io/images/yesterdaykite/post/d4cc4ba2-d8ef-46a6-aae7-be4bc001f5e3/image.png)


### ✔️ Observation

- the amount of uncertainty introduced into the system / cannot be greater than the key uncertainty.

- K uncertanity is at least M uncertainty
__(K엔트로피 >= M엔트로피) =  M info is perfectly concealed
__
occurs when all M's are equally probable (i.e. totally random)


## ✨ equivocation(모호성)
### ✔️ Definition
__Conditional entropies__ of K and M
(i.e., unpredictability of the K and M given an intercepted C.)
![](https://images.velog.io/images/yesterdaykite/post/186a88b1-9a29-4378-98b7-aa33c6c4869d/image.png)

### ✔️ Properties
> N = intercepted된 C 일부분 길이
__H(K, N|C) is a non-increasing function of N__

![](https://images.velog.io/images/yesterdaykite/post/d47c1523-7d9a-403a-864a-69c15d525537/image.png)
i.e., it's theoretically easier to determine the key as more C is intercepted. C가 많이 intercept될수록 K를 determine 하기 쉬워짐


### ✔️ For perfect secrecy!
__Mutual Info__
	- I(M|C) = C가 주어졌을때 M에 대해 얻을수있는 정보의 양
    - I(K|C) = C가 주어졌을때 K에 대해 얻을 수 있는 정보의 양

> I(M|C) = H(M) - H(M|C)

"new info revealed knowing C"
= "uncertainty" - "uncertainty after knowing C"


__How to achieve perfect secrecy?__
![](https://images.velog.io/images/yesterdaykite/post/cbe1eb6f-2387-4332-a3e4-d3c5d819513a/image.png)

> I(M|C) = H(M) - H(M|C) = 0
__H(M) = H(M|C)__

= __C and M are statistically independent__


also
> __H(M) <= H(K)__
= 적어도 K엔트로피는 M엔트로피보다 커야!
= num of M bits <= num of K bits
since..
![](https://images.velog.io/images/yesterdaykite/post/d552ba62-c47f-4650-8581-d0d559f7d398/image.png)

otherwise,,
like H(M) > H(K) !
Cryptanalyst __can obtain a lot of info__ about plaintext.
![](https://images.velog.io/images/yesterdaykite/post/7127323c-2d40-427d-83b5-77d2ab2f1c3a/image.png)


